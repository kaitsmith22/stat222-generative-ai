{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1434bb0-1609-48c9-9b1e-337fa58bac10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os\n",
    "import copy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from masked_celeba import CelebA\n",
    "from models.BasicAutoEncoder import BasicAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59ec9117-e34d-47dc-8507-3c9e24eda41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66ba4990-8a02-4fb8-b689-414407c18e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access celebA\n",
    "data_root = 'data'\n",
    "# Spatial size of training images, images are resized to this size.\n",
    "image_size = 28\n",
    "\n",
    "celeba_train = CelebA(data_root,\n",
    "                              download=False,\n",
    "                              split = \"train\",\n",
    "                              transform=transforms.Compose([\n",
    "                                  transforms.Grayscale(),\n",
    "                                  transforms.Resize(image_size),\n",
    "                                  transforms.CenterCrop(image_size),\n",
    "                                  transforms.ToTensor(),\n",
    "                              ]),\n",
    "                              proportion = 0.3)\n",
    "\n",
    "celeba_test = CelebA(data_root,\n",
    "                              download=False,\n",
    "                              split = \"test\",\n",
    "                              transform=transforms.Compose([\n",
    "                                  transforms.Grayscale(),\n",
    "                                  transforms.Resize(image_size),\n",
    "                                  transforms.CenterCrop(image_size),\n",
    "                                  transforms.ToTensor(),\n",
    "                              ]),\n",
    "                              proportion = 0.3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3ecfae2-1980-4d65-95af-e0caedb0a758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test dataloaders\n",
    "\n",
    "num_workers = 4\n",
    "# how many samples per batch to load\n",
    "batch_size = 2\n",
    "\n",
    "NUM_SAMP = 32\n",
    "\n",
    "# use random sampler to decrease training and testing time\n",
    "rand_sampler_train = torch.utils.data.RandomSampler(celeba_train, num_samples= NUM_SAMP, replacement=True)\n",
    "rand_sampler_test = torch.utils.data.RandomSampler(celeba_test, num_samples=NUM_SAMP, replacement=True)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(celeba_train, batch_size=batch_size, num_workers=num_workers, sampler=rand_sampler_train)\n",
    "val_loader = torch.utils.data.DataLoader(celeba_test, batch_size=batch_size, num_workers=num_workers, sampler = rand_sampler_test)\n",
    "\n",
    "dataloaders = {\"train\": train_loader,\n",
    "                   \"val\": val_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b560e90-59cb-485d-af26-b0fd26b74e3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CelebA' object has no attribute '_CelebA__getitem'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [18], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# display a couple of the test images and generated masks\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m3\u001B[39m):    \n\u001B[1;32m----> 4\u001B[0m     \u001B[43mceleba_train\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdisplay\u001B[49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Cal\\stat222\\stat222-generative-ai\\masked_celeba.py:84\u001B[0m, in \u001B[0;36mCelebA.display\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m     79\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdisplay\u001B[39m(\u001B[38;5;28mself\u001B[39m, index: \u001B[38;5;28mint\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     80\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     81\u001B[0m \u001B[38;5;124;03m        Display the masked and ground truth image at an index \u001B[39;00m\n\u001B[0;32m     82\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 84\u001B[0m     mask, X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__getitem\u001B[49m(index)\n\u001B[0;32m     86\u001B[0m     fig, (ax1, ax2) \u001B[38;5;241m=\u001B[39m plt\u001B[38;5;241m.\u001B[39msubplots(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m     87\u001B[0m     ax1\u001B[38;5;241m.\u001B[39mset_title(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMask\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'CelebA' object has no attribute '_CelebA__getitem'"
     ]
    }
   ],
   "source": [
    "# display a couple of the test images and generated masks\n",
    "for i in range(1,3):\n",
    "    celeba_train.display(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edb53b24-281a-4a69-86d3-855687f0df5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = torch.flatten(y_true)\n",
    "    y_pred_f = torch.flatten(y_pred)\n",
    "    intersection = torch.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection) / (torch.sum(y_true_f + y_pred_f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa6023a3-e59b-4756-9e5e-c07554567067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "BasicAE(\n  (encoder): Sequential(\n    (0): Linear(in_features=784, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=128, out_features=64, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=64, out_features=36, bias=True)\n    (5): ReLU()\n    (6): Linear(in_features=36, out_features=18, bias=True)\n    (7): ReLU()\n    (8): Linear(in_features=18, out_features=9, bias=True)\n  )\n  (decoder): Sequential(\n    (0): Linear(in_features=9, out_features=18, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=18, out_features=36, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=36, out_features=64, bias=True)\n    (5): ReLU()\n    (6): Linear(in_features=64, out_features=128, bias=True)\n    (7): ReLU()\n    (8): Linear(in_features=128, out_features=784, bias=True)\n    (9): Sigmoid()\n  )\n)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize model\n",
    "model = BasicAE()\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a60a163-0d21-4bc3-bc0a-6087702d11c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# specify loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, dataloaders,\n",
    "                device, track_out, num_epochs=8):\n",
    "    \"\"\"\n",
    "\n",
    "    :param model: initialized model to train\n",
    "    :param criterion: loss function\n",
    "    :param dataloaders: dictionary of dataloaders for training and validation\n",
    "    :param device: CPU or GPU to train on\n",
    "    :param track_out: whether or not to track the running accuracy and loss (for later plotting)\n",
    "    :param num_epochs:\n",
    "    :return: model, optimizer, best accuracy, best loss (optionally also running training/validation\n",
    "    los and accuracy)\n",
    "    \"\"\"\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_optim = copy.deepcopy(optimizer.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    if track_out:\n",
    "        train_loss = []\n",
    "        train_acc = []\n",
    "\n",
    "        val_loss = []\n",
    "        val_acc = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for image, mask in dataloaders[phase]:\n",
    "                # change this!\n",
    "                # this is currenlty needed to reshape images into input for BasicAE,\n",
    "                # but ideally I would just change the model architecture\n",
    "                image = image.reshape(-1, 28*28)\n",
    "                mask = mask.reshape(-1, 28*28)\n",
    "\n",
    "                image = image.to(device)\n",
    "                mask = mask.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(mask)\n",
    "                    loss = criterion(outputs, image)\n",
    "\n",
    "                    # propagate loss and step optimizer if in training stage\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # get loss\n",
    "                running_loss += loss.item() * image.size(0)\n",
    "                # change to DICE\n",
    "                # running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            # update the learning rate if this is training iteration\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / NUM_SAMP\n",
    "            # epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            if track_out:\n",
    "                if phase == \"train\":\n",
    "                    train_loss.append(epoch_loss.detach().cpu().squeeze())\n",
    "                    # train_acc.append(epoch_acc.detach().cpu().squeeze())\n",
    "\n",
    "                else:\n",
    "                    val_loss.append(epoch_loss.detach().cpu().squeeze())\n",
    "                    # val_acc.append(epoch_acc.detach().cpu().squeeze())\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f}')\n",
    "\n",
    "            # save model if accuracy has improved\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                # best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                best_optim = copy.deepcopy(optimizer.state_dict())\n",
    "                best_loss = epoch_loss\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    optimizer.load_state_dict(best_optim)\n",
    "\n",
    "    if track_out:\n",
    "        return model, optimizer, best_acc, best_loss, train_acc, train_loss, val_acc, val_loss\n",
    "\n",
    "    return model, optimizer, best_acc, best_loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "train Loss: 0.0700\n",
      "val Loss: 0.0690\n",
      "Epoch 1/9\n",
      "----------\n",
      "train Loss: 0.0747\n",
      "val Loss: 0.0649\n",
      "Epoch 2/9\n",
      "----------\n",
      "train Loss: 0.0644\n",
      "val Loss: 0.0749\n",
      "Epoch 3/9\n",
      "----------\n",
      "train Loss: 0.0691\n",
      "val Loss: 0.0610\n",
      "Epoch 4/9\n",
      "----------\n",
      "train Loss: 0.0644\n",
      "val Loss: 0.0828\n",
      "Epoch 5/9\n",
      "----------\n",
      "train Loss: 0.0702\n",
      "val Loss: 0.0647\n",
      "Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.0744\n",
      "val Loss: 0.0725\n",
      "Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.0669\n",
      "val Loss: 0.0714\n",
      "Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.0715\n",
      "val Loss: 0.0745\n",
      "Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.0606\n",
      "val Loss: 0.0667\n",
      "Training complete in 3m 59s\n",
      "Best val Acc: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "\n",
    "model_ft, optimizer_ft, accuracy, loss = train_model(model,\n",
    "                                                         criterion,\n",
    "                                                         optimizer,\n",
    "                                                         exp_lr_scheduler,\n",
    "                                                         dataloaders,\n",
    "                                                         device,\n",
    "                                                         track_out= False,\n",
    "                                                         num_epochs=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def show_validation(validation_loader):\n",
    "\n",
    "    dataiter = iter(validation_loader)\n",
    "    images, masks = next(dataiter)\n",
    "\n",
    "    masks = masks.reshape(-1, 28*28)\n",
    "\n",
    "    # get sample outputs\n",
    "    masks = masks.to(device)\n",
    "    torch.set_grad_enabled(False)\n",
    "    output = model(masks)\n",
    "\n",
    "    # prep images for display\n",
    "    masks = masks.cpu().numpy().reshape(-1, 28, 28)\n",
    "\n",
    "\n",
    "\n",
    "    # output is resized into a batch of images\n",
    "    output = output.cpu().numpy().reshape(-1, 28, 28)\n",
    "\n",
    "    for out, m in zip(output, masks):\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        ax1.set_title('Mask')\n",
    "        ax1.imshow(m[0])\n",
    "        ax2.set_title('Transformed Image')\n",
    "        ax2.imshow(out[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "show_validation(val_loader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
